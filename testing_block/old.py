####################################################################################################
# PROGRAM NAME: Research Agent Workflow (LLM Enhanced Agents) - COMBINED DEBUG CODE
# --------------------------------------------------------------------------------------------------
# DESCRIPTION: LangGraph state machine with production-ready API calls and ENHANCED DEBUGGING.
#              FIXES INCLUDED (v7.7.3 Review):
#              1. Supervisor routing logic fixed (recursion deadlock resolved by rag_complete flag).
#              2. QueryGenAgent and MaterialsAgent data flow corrected (using elements list heuristic).
#              3. WebAgent call error resolved (DuckDuckGo Search call fixed).
# AUTHOR: AI Agent Team + Gemini Review (Debug Prints Integrated)
# DATE: 2025-12-05 (Consolidated and Fixed)
# VERSION: 7.7.4 (Stable Release Candidate)
####################################################################################################

# ==================================================================================================
# SECTION 0: IMPORTS AND CONFIGURATION
# ==================================================================================================
import os
import sys
import io
import time
import json
import pickle
import traceback
import ast
import re
import numpy as np
import faiss
import arxiv
import requests
from io import BytesIO
from typing import TypedDict, Any, Dict, List, Optional, Callable, Tuple
from dotenv import load_dotenv
from pypdf import PdfReader
from openai import OpenAI
from mp_api.client import MPRester
from langgraph.graph import StateGraph, END
from graphviz import Digraph
from Bio import Entrez
from ddgs import DDGS
from datetime import datetime
import operator
#---- evolution model -----------
from pydantic import BaseModel, Field
#-----------------------------------
from langchain_openai import ChatOpenAI
#------ evolution agent ----------------

# --- ANSI Color Codes ---
C_RESET = "\033[0m"
C_RED = "\033[91m"
C_GREEN = "\033[92m"  # Success/Done
C_YELLOW = "\033[93m" # Data flow/State update/DEBUG
C_BLUE = "\033[94m"  # Agent Info
C_MAGENTA = "\033[95m" # Router/Supervisor
C_CYAN = "\033[96m"  # Initialization/Setup
C_ACTION = "\033[38;5;208m" # Action/Start

# Load Environment Variables
print(f"{C_CYAN} >> [INIT] Loading environment variables...{C_RESET}")
load_dotenv()

OPENAI_API_KEY = os.getenv("GPT_API_KEY")
MP_API_KEY = os.getenv("MP_API_KEY")
EMBED_MODEL = "text-embedding-3-small"
#LLM_MODEL = "gpt-3.5-turbo-0125"
LLM_MODEL = "gpt-4-turbo-2024-04-09"

if not OPENAI_API_KEY:
    print(f"{C_RED} >> [FATAL] GPT_API_KEY not found. Aborting.{C_RESET}")
    print(f"{C_RED} >> WARNING: Proceeding without API Key. LLM and Tool Agents will fail.{C_RESET}")
    client = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)
    print(f"{C_CYAN} >> [INIT] OpenAI client initialized successfully.{C_RESET}")

# RAG Utility Constants
DIMENSION = 1536
VECTOR_INDEX_PATH = "vector_index.faiss"
VECTOR_DATA_PATH = "vector_data.pkl"

# Entrez Email (Required by Bio.Entrez for PubMed)
Entrez.email = "research.agent@example.com"


# ==================================================================================================
# CLEAN RESEARCH STATE FOR MULTI-AGENT SYSTEM (MODIFIED)
# ==================================================================================================
class ResearchState(TypedDict):
    """
    Central shared memory for the Supervisor-driven multi-agent research workflow,
    including fields for quality evaluation and refinement loops.
    """

    # --- User Inputs & Planning ---
    user_query: str                     # Original query from the user
    semantic_query: str                 # Processed / cleaned / normalized user query
    primary_intent: str                 # Classified intent (e.g., material, disease)
    execution_plan: List[str]           # High-level plan generated by LLM
    material_elements: List[str]        # Extracted elements (e.g., ['Li', 'Co', 'O'])
    api_search_term: str                # Specific term used for MP search (e.g., LiCoO2)
    tiered_queries: Dict[str, Dict[str, str]]   # strict/moderate/broad queries for each tool
    active_tools: List[str]             # <<<<<<<< NEW: Tools selected by the Planning Agent

    # --- Tool and Retrieval Data ---
    raw_tool_data: List[Dict[str, Any]]         # All raw outputs from all tool agents (abstracts, web snippets, etc.)
    full_text_chunks: List[Dict[str, Any]]      # Extracted + smart-chunked PDF content

    # --- RAG & Control Flags ---
    rag_complete: Optional[bool]        # Signals that the RAG pipeline has finished executing.
    filtered_context: str               # Semantic + keyword-filtered context ready for synthesis
    references: List[str]               # Citations gathered during the Retrieval/RAG steps

    # --- Finalization & Evaluation Loop ---
    final_report: str                   # The comprehensive report produced by the synthesis agent
    report_generated: bool              # Flag set by SynthesisAgent
    needs_refinement: bool              # CRITICAL FLAG: Set by EvaluationAgent.
    refinement_reason: str              # Feedback from the Evaluation LLM.
    is_refining: bool                   # Set to True when looping back from evaluation

    # Internal routing/control fields
    next: str                           # Key used by the graph to determine the next node or 'TERMINATE'

# ==================================================================================================
# SECTION 1: SUPERVISOR AGENT (PROCEDURAL ROUTER) - FINALIZED FOR DYNAMIC ROUTING
# ==================================================================================================
class SupervisorAgent:
    """
    Central coordinator of the fully dynamic multi-agent research workflow,
    managing the Synthesis-Evaluation-Refinement loop and dynamic tool selection.
    """
    ALL_TOOL_KEYS = ["pubmed", "arxiv", "openalex", "web", "materials"] # For global initialization

    def __init__(self, max_retries: int = 2):
        self.max_retries = max_retries
        self.tool_status: Dict[str, str] = {tool: "PENDING" for tool in self.ALL_TOOL_KEYS}
        self.tool_retries = {tool: 0 for tool in self.ALL_TOOL_KEYS}

        self.retrieval_retries = 0
        self.refinement_retries = 0

    def select_next_agent(self, state: ResearchState) -> Optional[str]:

        print(f"\n{C_MAGENTA}[{self.__class__.__name__.upper()} ROUTER] Assessing State for Next Action...{C_RESET}")

        # 1. FINAL TERMINATION CHECK (Evaluation/Refinement Loop)
        if state.get("report_generated", False):
            print(f"\n{C_MAGENTA}[SUPERVISOR ROUTE] Received state from Evaluation Node.{C_RESET}")
            print(f"{C_MAGENTA}[SUPERVISOR ROUTE] needs_refinement flag: {state.get('needs_refinement', 'N/A')}{C_RESET}")

            MAX_REFINEMENT_ATTEMPTS = 2

            if not state.get("needs_refinement", True):
                self.retrieval_retries = 0
                self.refinement_retries = 0
                print(f"{C_MAGENTA}[ROUTING] State: Evaluation passed. -> FINISH{C_RESET}")
                return None

            else:
                self.refinement_retries += 1
                if self.refinement_retries > MAX_REFINEMENT_ATTEMPTS:
                    print(f"{C_RED}[ROUTING FAIL] Refinement loop maxed out. Forcing FINISH.{C_RESET}")
                    self.refinement_retries = 0
                    return None

                state['report_generated'] = False
                state['is_refining'] = True

                # âœ… CORRECTED ROUTE: Route directly to the Synthesis Agent for the rewrite.
                print(f"{C_MAGENTA}[ROUTING] Refinement required. Attempt {self.refinement_retries}/{MAX_REFINEMENT_ATTEMPTS}. -> synthesis_agent{C_RESET}")
                return "synthesis_agent" # <-- THE CRITICAL FIX

        # 2. Initial Setup Checks (Sequential)
        if not state.get("tiered_queries"):
            if not state.get("semantic_query"): return "clean_query_agent"
            if not state.get("primary_intent"): return "intent_agent"

            if not state.get("execution_plan") or not state.get("active_tools"):
                return "planning_agent"

            return "query_generation_agent"

        # 3. Tool Execution Checks (Parallel/Iterative) - DYNAMIC ROUTING CORE

        active_tools = state.get("active_tools", [])

        print(f"{C_MAGENTA}[ROUTING] Active Tools for Execution: {active_tools}{C_RESET}")

        # Iterate ONLY over the tools selected by the Planning Agent
        for tool in active_tools:
            tool_agent_id = f"{tool}_agent"

            # Check 1: Tool failed to get input from QueryGen (e.g., missing query)
            if tool not in state.get('tiered_queries', {}) and tool != "materials":
                 print(f"{C_YELLOW}[ROUTING WARN] Tool '{tool}' is active, but tiered queries are missing. Skipping.{C_RESET}")
                 self.tool_status[tool] = "FAILED"
                 continue

            # Check 2: Tool has already executed successfully and has data
            tool_results = [
                d for d in state.get("raw_tool_data", [])
                if d.get("tool_id") == tool_agent_id
            ]

            if tool_results:
                self.tool_status[tool] = "SUCCESS"
                continue

            # Check 3: Tool has maxed out its retries
            if self.tool_retries.get(tool, 0) > self.max_retries:
                 self.tool_status[tool] = "FAILED"
                 continue

            # Check 4: Tool is skippable (e.g., Materials Agent without valid formula)
            is_skippable = False
            if tool == "materials" and not state.get("material_elements", []):
                is_skippable = True

            if is_skippable:
                print(f"{C_YELLOW}[ROUTING SKIP] Tool {tool} is skippable (missing required input).{C_RESET}")
                self.tool_status[tool] = "FAILED"
                continue

            # Check 5: EXECUTE PENDING TOOL (The critical step)
            if self.tool_status[tool] == "PENDING":
                self.record_tool_retry(tool)
                print(f"{C_MAGENTA}[ROUTING] Next Tool to run: {tool_agent_id}{C_RESET}")
                return tool_agent_id # Return the tool agent ID to the graph


        # 4. Retrieval and RAG Checks (Post-Search)
        MAX_RETRIEVAL_ATTEMPTS = 5

        # Only run retrieval if we have raw data and haven't created chunks yet
        if state.get("raw_tool_data") and not state.get("full_text_chunks"):

            # Safety break for the retrieval loop
            if self.retrieval_retries >= MAX_RETRIEVAL_ATTEMPTS:
                print(f"{C_RED}[ROUTING FAIL] Retrieval Agent failed multiple times. Forcing skip to RAG.{C_RESET}")
                self.retrieval_retries = 0
                state['full_text_chunks'] = ["No full-text could be extracted; synthesizing with abstracts/snippets only."]
                return "rag_agent"

            self.retrieval_retries += 1
            print(f"{C_MAGENTA}[ROUTING] State: Raw data present, needs chunking. -> retrieval_agent{C_RESET}")
            return "retrieval_agent"

        # Reset retrieval retries after the chunking is successful
        if state.get("full_text_chunks") and self.retrieval_retries > 0:
            self.retrieval_retries = 0

        if state.get("full_text_chunks") and not state.get("rag_complete", False):
            print(f"{C_MAGENTA}[ROUTING] State: Chunks present, needs filtering/RAG. -> rag_agent{C_RESET}")
            return "rag_agent"


        # 5. Synthesis Check
        if state.get("rag_complete", False) and not state.get("report_generated", False):
            print(f"{C_MAGENTA}[ROUTING] State: Context ready, needs final report. -> synthesis_agent{C_RESET}")
            return "synthesis_agent"

        # 6. Final Fallback
        print(f"{C_RED}[ROUTING] FALLBACK: All active tool executions complete. Routing to FINISH.{C_RESET}")
        return None

    # -------------------------
    # Tool retry management
    # -------------------------
    def record_tool_retry(self, tool_name: str):
        if tool_name in self.tool_retries:
            self.tool_retries[tool_name] += 1
            print(f"{C_YELLOW}[{self.__class__.__name__}] Recorded retry for {tool_name}. Total: {self.tool_retries[tool_name]}{C_RESET}")

def supervisor_placeholder(state: ResearchState) -> ResearchState:
    """A required node function that simply passes the state to the router."""
    return state

# ==================================================================================================
# SECTION 2: CLEAN QUERY AGENT (PROCEDURAL)
# ==================================================================================================
class CleanQueryAgent:
    """
    Agent responsible for cleaning the user's query and generating a semantic query.
    """
    def __init__(self):
        self.id = "clean_query_agent"

    def run(self, state: ResearchState) -> ResearchState:
        """
        Processes the user query and fills the 'semantic_query' in the state.
        """
        user_query = state.get("user_query", "").strip()
        print(f"USER_QUERY:{user_query}")
        print(f"\n{C_ACTION}[{self.id.upper()} START] Cleaning initial query: '{user_query[:50]}...'{C_RESET}")

        cleaned_query = " ".join(user_query.split())
        semantic_query = self.generate_semantic_query(cleaned_query)

        state["user_query"] = cleaned_query
        state["semantic_query"] = semantic_query

        print(f"{C_YELLOW}[{self.id.upper()} STATE] Updated semantic_query: **'{semantic_query[:50]}...'** {C_RESET}")
        print(f"{C_GREEN}[{self.id.upper()} DONE] Query cleaning complete.{C_RESET}")
        return state

    def generate_semantic_query(self, query: str) -> str:
        return query


# ==================================================================================================
# SECTION 3: INTENT AGENT (LLM-Driven)
# ==================================================================================================
class IntentAgent:
    """
    Agent responsible for determining the primary intent of the user query using an LLM.
    """
    def __init__(self, model: str = LLM_MODEL):
        self.id = "intent_agent"
        self.model = model
        self.valid_intents = [
            "literature_review",
            "materials_research",
            "comparative_analysis",
            "medical_diagnosis",
            "general_research",
            "data_extraction"
        ]

    def _format_prompt(self, semantic_query: str) -> str:
        intent_list_str = ", ".join([f"'{i}'" for i in self.valid_intents])
        return f"""
        Analyze the following research query and classify its primary intent.

        **Semantic Query:** "{semantic_query}"

        **Instructions:**
        1. Choose the single best classification from this list: [{intent_list_str}].
        2. Provide a brief, one-sentence description of why you chose that intent.

        **Output MUST be a single JSON object (Dict) with two keys: 'primary_intent' (string) and 'reasoning' (string).**
        """

    def _call_llm_and_parse(self, prompt: str) -> Optional[Dict[str, Any]]:
        if client is None: return None
        try:
            print(f"{C_BLUE}[{self.id} ACTION] Calling LLM ({self.model}) to classify intent...{C_RESET}")
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert research intent classifier. Output ONLY the requested JSON object."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            json_str = response.choices[0].message.content.strip()
            llm_output = json.loads(json_str)

            print(f"{C_YELLOW}[{self.id.upper()} DEBUG] Raw LLM Output: {llm_output}{C_RESET}")

            return llm_output

        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] LLM call or JSON parsing failed: {e}{C_RESET}")
            return None

    def run(self, state: ResearchState) -> ResearchState:
        print(f"\n{C_ACTION}[{self.id.upper()} START] Determining intent with **LLM Classification**...{C_RESET}")
        semantic_query = state.get("semantic_query", "")

        llm_output = self._call_llm_and_parse(self._format_prompt(semantic_query))

        if llm_output and llm_output.get("primary_intent"):
            primary_intent = llm_output["primary_intent"]
            if primary_intent not in self.valid_intents:
                print(f"{C_YELLOW}[{self.id} WARN] LLM returned invalid intent: '{primary_intent}'. Defaulting to 'general_research'.{C_RESET}")
                primary_intent = "general_research"

            state["primary_intent"] = primary_intent

            print(f"{C_YELLOW}[{self.id.upper()} STATE] Determined primary_intent: **{primary_intent}** (Reason: {llm_output.get('reasoning', 'N/A')[:50]}...){C_RESET}")
            print(f"{C_GREEN}[{self.id.upper()} DONE] Intent determined by LLM.{C_RESET}")
        else:
            state["primary_intent"] = "general_research"
            print(f"{C_RED}[{self.id.upper()} FAIL] LLM intent classification failed. Defaulting to 'general_research'.{C_RESET}")

        return state


# ==================================================================================================
# SECTION 4: PLANNING AGENT (LLM-Driven) - UPGRADED FOR DYNAMIC TOOL SELECTION
# ==================================================================================================
class PlanningAgent:
    """
    Agent responsible for creating an execution plan AND dynamically selecting
    the necessary tools based on the primary intent and semantic query.
    """
    def __init__(self, model: str = LLM_MODEL):
        self.id = "planning_agent"
        self.model = model
        self.available_tools = [
            "pubmed", "arxiv", "openalex", "web", "materials" # List of all possible tool keys
        ]

    def _format_prompt(self, intent: str, query: str) -> str:
        tool_list_str = ", ".join([f"'{t}'" for t in self.available_tools])

        return f"""
        You are an expert project manager for a multi-agent research system. Based on the user's intent and query,
        generate a detailed, step-by-step execution plan (minimum 5 steps) AND dynamically select the minimal set of tools required.

        **Primary Intent:** {intent}
        **Semantic Query:** "{query}"

        **Available Tools (Choose a subset):** {tool_list_str}

        **Tool Selection Guidelines:**
        1. If the query involves specific material properties (bandgap or stability), include **'materials'**.
        2. If the query is a literature review, include **'pubmed'** and **'arxiv'**.
        3. For general background or current events, include **'web'**.
        4. Select only the tools absolutely necessary for the query.

        **Instructions:**
        1. **Plan Detail:** Each step must be concise but actionable.
        2. **Output Format:** Output ONLY a single JSON object with two required keys.
           - 'execution_plan': (list of strings) The step-by-step plan.
           - 'active_tools': (list of strings) The selected tools.
        """

    def _call_llm_and_parse(self, prompt: str) -> Optional[Dict[str, Any]]:
        if client is None: return None
        try:
            print(f"{C_BLUE}[{self.id} ACTION] Calling LLM ({self.model}) to generate execution plan and select tools...{C_RESET}")
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert research planner. Output ONLY the requested JSON object."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            json_str = response.choices[0].message.content.strip()
            return json.loads(json_str)

        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] LLM call or JSON parsing failed: {e}{C_RESET}")
            return None

    def run(self, state: ResearchState) -> ResearchState:
        print(f"\n{C_ACTION}[{self.id.upper()} START] Generating plan and selecting tools (LLM Planning)...{C_RESET}")
        primary_intent = state.get("primary_intent", "general_research")
        semantic_query = state.get("semantic_query", "")

        llm_output = self._call_llm_and_parse(self._format_prompt(primary_intent, semantic_query))

        if llm_output and isinstance(llm_output, dict):
            execution_plan = llm_output.get("execution_plan", [])
            active_tools = llm_output.get("active_tools", [])

            validated_tools = [t for t in active_tools if t in self.available_tools]

            if not validated_tools:
                 print(f"{C_RED}[{self.id.upper()} FAIL] LLM failed to select valid tools. Using procedural fallback.{C_RESET}")
                 validated_tools = ["pubmed", "arxiv", "web"]

            if not execution_plan:
                 execution_plan = ["Execute selected tool searches.", "Generate a final synthesis report."]

            state["execution_plan"] = execution_plan
            state["active_tools"] = validated_tools # <-- SETTING THE DYNAMIC LIST

            print(f"{C_YELLOW}[{self.id.upper()} STATE] Plan generated. Active Tools selected: **{validated_tools}**{C_RESET}")

        else:
            state["execution_plan"] = ["Execute multi-tool searches.", "Generate final synthesis report."]
            state["active_tools"] = ["pubmed", "arxiv", "web", "materials"]
            print(f"{C_RED}[{self.id.upper()} FAIL] LLM planning/selection failed. Using fallback plan/tools.{C_RESET}")

        print(f"{C_GREEN}[{self.id.upper()} DONE] Plan and Tool selection complete.{C_RESET}")
        return state


# ==================================================================================================
# SECTION 5: TOOL AGENTS (PROCEDURAL)
# ==================================================================================================
# ==================================================================================================
# SECTION 5.1: PUBMED AGENT
# ==================================================================================================
class PubMedAgent:
    """Queries PubMed using tiered searches via Entrez."""
    def __init__(self, agent_id: str = "pubmed_agent", min_results: int = 5):
        self.id = agent_id
        self.min_results = min_results
        self.query_order = ["strict", "moderate", "broad"]

        # CRITICAL: Entrez requires a registered email
        Entrez.email = "your.email@example.com" # !!! REPLACE WITH REAL EMAIL !!!

    def _execute_tiered_search(self, tiered_queries: Dict[str, str]) -> List[str]:
        pmids: List[str] = []
        for tier in self.query_order:
            current_query = tiered_queries.get(tier)
            if not current_query or not current_query.strip(): continue
            print(f"[{self.id} SEARCH] Trying '{tier}' query: '{current_query[:50]}...'")
            try:
                handle = Entrez.esearch(db="pubmed", term=current_query, retmax=self.min_results, sort="relevance")
                ids = Entrez.read(handle).get("IdList", [])
                handle.close()
                if ids:
                    pmids = ids
                    return pmids
            except Exception as e:
                print(f"{C_RED}[{self.id} FAIL] Tier '{tier}' failed: {e}.{C_RESET}")
        return []

    def _fetch_metadata_for_pmids(self, pmids: List[str]) -> List[Dict[str, Any]]:
        if not pmids: return []
        metadata_list = []
        try:
            # Use 'xml' mode for fetching structured data
            handle = Entrez.efetch(db="pubmed", id=pmids, rettype="medline", retmode="xml")
            records = Entrez.read(handle)
            handle.close()
            for record in records.get('PubmedArticle', []):
                article = record.get('MedlineCitation', {}).get('Article', {})
                pmid = str(record.get('MedlineCitation', {}).get('PMID', 'N/A'))
                title = str(article.get('ArticleTitle', 'No Title')).strip()
                abstract_sections = article.get('Abstract', {}).get('AbstractText', [])
                abstract = " ".join([str(s) for s in abstract_sections]).strip()
                text_content = f"Title: {title}. Abstract: {abstract}"
                metadata_list.append({
                    'text': text_content,
                    'source_type': 'pubmed',
                    'tool_id': self.id,
                    'metadata': {'pmid': pmid, 'title': title, 'abstract': abstract}
                })
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] Metadata fetch failed: {e}{C_RESET}")
            return []
        return metadata_list

    def execute(self, state: ResearchState) -> ResearchState:
        # --- DYNAMIC GUARDRAIL ---
        expected_id = self.id.replace("_agent", "") # 'pubmed'
        if expected_id not in state.get("active_tools", []):
            print(f"{C_YELLOW}[{self.id.upper()} SKIP] Tool '{expected_id}' not in active_tools list. Skipping execution.{C_RESET}")
            return state
        # -------------------------

        print(f"\n{C_ACTION}[{self.id.upper()} START] Executing tiered PubMed search (Entrez API)...{C_RESET}")
        try:
            pubmed_queries = state['tiered_queries']['pubmed']
        except KeyError:
            print(f"{C_RED}[{self.id} FAIL] No PubMed queries in state. Skipping.{C_RESET}")
            return state

        pmids = self._execute_tiered_search(pubmed_queries)
        if pmids:
            standardized_data = self._fetch_metadata_for_pmids(pmids)
            state.setdefault('raw_tool_data', []).extend(standardized_data)
            state.setdefault('references', []).extend([f"ðŸ“„ Journal Article: {r['metadata']['title']}" for r in standardized_data])
            print(f"{C_GREEN}[{self.id.upper()} DONE] Added {len(standardized_data)} PubMed abstracts to raw_tool_data.{C_RESET}")
        else:
            print(f"{C_RED}[{self.id.upper()} WARNING] No relevant results retrieved.{C_RESET}")
        return state

# ==================================================================================================
# SECTION 5.2: ARXIV AGENT
# ==================================================================================================

class ArxivAgent:
    """Queries ArXiv using tiered searches."""
    def __init__(self, agent_id: str = "arxiv_agent", min_results: int = 3):
        self.id = agent_id
        self.min_results = min_results
        self.query_order = ["strict", "moderate", "broad"]

    def _call_arxiv_search(self, term: str) -> List[arxiv.Result]:
        results = []
        try:
            search = arxiv.Search(
                query=term,
                max_results=self.min_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            client_arxiv = arxiv.Client()
            # Exhaust the generator into a list
            for r in client_arxiv.results(search):
                results.append(r)
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] ArXiv API call failed: {str(e)}{C_RESET}")
            return []
        return results

    def _standardize_arxiv_results(self, raw_results: List[Any]) -> List[Dict[str, Any]]:
        standardized_list = []
        for r in raw_results:
            summary_snippet = getattr(r, 'summary', 'No summary')[:200].replace('\n', ' ')
            text_content = f"Title: {r.title}. Abstract: {summary_snippet}"
            standardized_list.append({
                'text': text_content,
                'source_type': 'arxiv',
                'tool_id': self.id,
                'metadata': {
                    'arxiv_id': r.entry_id,
                    'title': r.title,
                    'abstract': summary_snippet,
                    'published_year': r.published.year,
                    'pdf_url': r.pdf_url
                }
            })
        return standardized_list

    def execute(self, state: ResearchState) -> ResearchState:
        # --- DYNAMIC GUARDRAIL ---
        expected_id = self.id.replace("_agent", "") # 'arxiv'
        if expected_id not in state.get("active_tools", []):
            print(f"{C_YELLOW}[{self.id.upper()} SKIP] Tool '{expected_id}' not in active_tools list. Skipping execution.{C_RESET}")
            return state
        # -------------------------

        print(f"\n{C_ACTION}[{self.id.upper()} START] Executing tiered ArXiv search (Arxiv API)...{C_RESET}")
        try:
            arxiv_queries = state['tiered_queries']['arxiv']
        except KeyError:
            print(f"{C_RED}[{self.id} FAIL] No ArXiv queries in state. Skipping.{C_RESET}")
            return state

        raw_results = []
        for tier in self.query_order:
            current_query = arxiv_queries.get(tier)
            if current_query and current_query.strip():
                raw_results = self._call_arxiv_search(current_query)
                if raw_results:
                    print(f"{C_GREEN}[{self.id} SUCCESS] Tier '{tier}' found {len(raw_results)} papers.{C_RESET}")
                    break

        if raw_results:
            standardized_data = self._standardize_arxiv_results(raw_results)
            state.setdefault('raw_tool_data', []).extend(standardized_data)
            state.setdefault('references', []).extend([f"ðŸ”— Arxiv: {r['metadata']['title']}" for r in standardized_data])
            print(f"{C_GREEN}[{self.id.upper()} DONE] Added {len(standardized_data)} ArXiv abstracts, including PDF URLs.{C_RESET}")
        return state

# ==================================================================================================
# SECTION 5.3: OPENALEX AGENT
# ==================================================================================================

class OpenAlexAgent:
    """Queries OpenAlex for works related to the research query."""
    def __init__(self, agent_id: str = "openalex_agent", max_results: int = 3):
        self.id = agent_id
        self.max_results = max_results
        self.base_url = "https://api.openalex.org/works"
        self.email_for_polite_pool = "mailto:your.email@example.com" # Best practice

    def _call_openalex_api(self, query: str) -> List[Dict[str, Any]]:
        try:
            # Added email for polite pool and filter for title search
            url = f"{self.base_url}?filter=title.search:{query}&per-page={self.max_results}&{self.email_for_polite_pool}"
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            return r.json().get("results", [])
        except requests.exceptions.RequestException as e:
            print(f"{C_RED}[{self.id} ERROR] OpenAlex request failed: {e}{C_RESET}")
            return []

    def _reconstruct_openalex_abstract(self, inverted_index: Dict[str, List[int]]) -> str:
        """Reconstructs the abstract from OpenAlex's inverted index format."""
        if not inverted_index: return ""
        # Find the maximum index to determine the array size
        max_index = max((max(indices) for indices in inverted_index.values() if indices), default=-1)
        if max_index == -1: return ""

        words = [None] * (max_index + 1)
        for word, indices in inverted_index.items():
            for index in indices:
                if index < len(words): words[index] = word
        # Join words, filtering out None placeholders if reconstruction was imperfect
        return " ".join([word for word in words if word is not None]).strip()

    def _standardize_openalex_results(self, raw_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        standardized_list = []
        for r in raw_results:
            title = r.get('title', 'No Title Available')
            # Use the reconstruction method
            abstract = self._reconstruct_openalex_abstract(r.get('abstract_inverted_index', {}))
            text_content = f"Title: {title}. Abstract: {abstract}"
            standardized_list.append({
                'text': text_content,
                'source_type': 'openalex',
                'tool_id': self.id,
                'metadata': {
                    'openalex_id': r.get('id', 'N/A'),
                    'title': title
                }
            })
        return standardized_list

    def execute(self, state: ResearchState) -> ResearchState:
        # --- DYNAMIC GUARDRAIL ---
        expected_id = self.id.replace("_agent", "") # 'openalex'
        if expected_id not in state.get("active_tools", []):
            print(f"{C_YELLOW}[{self.id.upper()} SKIP] Tool '{expected_id}' not in active_tools list. Skipping execution.{C_RESET}")
            return state
        # -------------------------

        print(f"\n{C_ACTION}[{self.id.upper()} START] Retrieving open-access data (OpenAlex API)...{C_RESET}")
        try:
            # We assume a 'simple' query tier for OpenAlex
            search_query = state['tiered_queries']['openalex'].get('simple')
            if not search_query or not search_query.strip():
                 raise KeyError("No simple query found for OpenAlex or query is empty.")
        except KeyError:
            print(f"{C_RED}[{self.id} FAIL] No OpenAlex query in state. Skipping.{C_RESET}")
            return state

        raw_results = self._call_openalex_api(search_query)
        if raw_results:
            standardized_data = self._standardize_openalex_results(raw_results)
            state.setdefault('raw_tool_data', []).extend(standardized_data)
            state.setdefault('references', []).extend([f"ðŸ”— OpenAlex: {r['metadata']['title']}" for r in standardized_data])
            print(f"{C_GREEN}[{self.id.upper()} DONE] Added {len(standardized_data)} OpenAlex works.{C_RESET}")
        return state

# ==================================================================================================
# SECTION 5.4: MATERIALs AGENT
# ==================================================================================================
class MaterialsAgent:
    """Queries Materials Project for material properties."""
    def __init__(self, agent_id: str = "materials_agent", max_results: int = 5):
        self.id = agent_id
        self.max_results = max_results

    def _call_materials_project_api(self, formula: str, max_results: int) -> List[Dict[str, Any]]:
        # Check for API key existence before proceeding
        if not globals().get('MP_API_KEY'):
            print(f"{C_RED}[{self.id} ERROR] MP_API_KEY is not set. Cannot execute Materials Project query.{C_RESET}")
            return []

        try:
            with MPRester(globals()['MP_API_KEY']) as mpr:
                fields = ["material_id", "formula_pretty", "is_stable", "band_gap", "energy_above_hull"]

                docs = mpr.materials.summary.search(
                    formula=formula, # Search strictly by formula
                    fields=fields,
                )

                results = []
                for d in docs:
                    d_dict = d.dict()
                    results.append({
                        "material_id": d_dict.get("material_id", "N/A"),
                        "formula": d_dict.get("formula_pretty", "N/A"),
                        "is_stable": d_dict.get("is_stable", False),
                        "band_gap": d_dict.get("band_gap", None),
                        "energy_above_hull": d_dict.get("energy_above_hull", None)
                    })

                return results[:max_results]
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] MP API call failed: {e}{C_RESET}")
            return []

    def _standardize_mp_results(self, raw_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        standardized_list = []
        for result in raw_results:
            stability_status = "Stable" if result['is_stable'] else f"Unstable (E/hull: {result['energy_above_hull']} eV)"
            band_gap_str = f"{result['band_gap']} eV" if result['band_gap'] is not None else "Metallic/Unknown"

            text_content = (
                f"Material: {result['formula']} ({result['material_id']}). "
                f"Stability: {stability_status}. "
                f"Band Gap: {band_gap_str}. "
                f"Energy Above Hull: {result['energy_above_hull']} eV."
            )
            standardized_list.append({
                'text': text_content,
                'source_type': 'materials_project',
                'tool_id': self.id,
                'metadata': {
                    'material_id': result['material_id'],
                    'formula': result['formula'],
                    'is_stable': result['is_stable'],
                    'band_gap': result['band_gap'],
                    'energy_above_hull': result['energy_above_hull'],
                }
            })
        return standardized_list

    def execute(self, state: ResearchState) -> ResearchState:
        # --- DYNAMIC GUARDRAIL ---
        expected_id = self.id.replace("_agent", "") # 'materials'
        if expected_id not in state.get("active_tools", []):
            print(f"{C_YELLOW}[{self.id.upper()} SKIP] Tool '{expected_id}' not in active_tools list. Skipping execution.{C_RESET}")
            return state
        # -------------------------

        print(f"\n{C_ACTION}[{self.id.upper()} START] Retrieving material properties (MPRester API)...{C_RESET}")
        extracted_elements = state.get("material_elements", [])

        # Internal check: Only proceed if a valid chemical formula is likely present
        if extracted_elements and re.match(r'^[A-Z][a-z]?\w*\d*$', extracted_elements[0]):
             target_formula = extracted_elements[0]
             state['api_search_term'] = target_formula
        else:
             print(f"{C_YELLOW}[{self.id} WARN] No valid compound formula found in elements. Skipping strict MP search.{C_RESET}")
             return state

        raw_results = self._call_materials_project_api(target_formula, self.max_results)

        if raw_results:
            standardized_data = self._standardize_mp_results(raw_results)
            state.setdefault('raw_tool_data', []).extend(standardized_data)
            state.setdefault('references', []).extend([f"âš›ï¸ Materials Project: {r['metadata']['material_id']} ({r['metadata']['formula']})" for r in standardized_data])
            print(f"{C_GREEN}[{self.id.upper()} DONE] Added {len(standardized_data)} material entries (max {self.max_results}) to raw_tool_data.{C_RESET}")

        return state

# ==================================================================================================
# SECTION 5.5: WEB AGENT
# ==================================================================================================

class WebAgent:
    """Performs web search via DuckDuckGo."""
    def __init__(self, agent_id: str = "web_agent"):
        self.id = agent_id

    def _call_ddg_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        try:
            with DDGS() as ddgs:
                # Use the text method for general search snippets
                return list(ddgs.text(query, max_results=limit))
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] DDGS search failed for query '{query[:40]}...': {e}{C_RESET}")
            return []

    def _standardize_web_results(self, raw_results: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        standardized_list = []
        for result in raw_results:
            # 'body' often contains the snippet/summary
            text_content = f"Title: {result.get('title', 'No Title')}. Snippet: {result.get('body', 'No Snippet')}"
            standardized_list.append({
                'text': text_content,
                'source_type': 'web_search',
                'tool_id': self.id,
                'metadata': {
                    'title': result.get('title'),
                    'url': result.get('href'),
                    'source_name': 'DuckDuckGo Search'
                }
            })
        return standardized_list

    def execute(self, state: ResearchState) -> ResearchState:
        # --- DYNAMIC GUARDRAIL ---
        expected_id = self.id.replace("_agent", "") # 'web'
        if expected_id not in state.get("active_tools", []):
            print(f"{C_YELLOW}[{self.id.upper()} SKIP] Tool '{expected_id}' not in active_tools list. Skipping execution.{C_RESET}")
            return state
        # -------------------------

        print(f"\n{C_ACTION}[{self.id.upper()} START] Retrieving general context (DuckDuckGo Search)...{C_RESET}")

        focused_query = None
        try:
            # We assume a 'simple' query tier for Web Agent
            focused_query = state['tiered_queries']['web'].get('simple')
            if not focused_query or not focused_query.strip():
                 raise KeyError("No simple query found for Web Agent or query is empty.")
        except KeyError:
             focused_query = state.get('semantic_query', 'research')

        raw_results = self._call_ddg_search(query=focused_query, limit=5)

        if raw_results:
            standardized_results = self._standardize_web_results(raw_results)
            state.setdefault('raw_tool_data', []).extend(standardized_results)
            state.setdefault('references', []).extend([f"ðŸ”— Web Source: {r['metadata']['title']} ({r['metadata']['url']})" for r in standardized_results])
            print(f"{C_GREEN}[{self.id.upper()} DONE] Added {len(standardized_results)} web snippets to raw_tool_data.{C_RESET}")
        else:
            print(f"{C_YELLOW}[{self.id} WARN] No web results found for query: '{focused_query[:40]}...'{C_RESET}")

        return state

# ==================================================================================================
# SECTION 6: QUERY GENERATION AGENT (LLM-Driven) - FIXES APPLIED HERE
# ==================================================================================================
class QueryGenerationAgent:
    """
    Agent responsible for using an LLM to generate tiered, tool-specific queries.
    """

    def __init__(self, agent_id: str = "query_gen_agent", model: str = LLM_MODEL):
        self.id = agent_id
        self.model = model

        self.search_tiers = {
            "pubmed": ["strict", "moderate", "broad"],
            "arxiv": ["strict", "moderate", "broad"],
            "openalex": ["simple"],
            "web": ["simple"],
        }

    def _format_prompt(self, semantic_query: str) -> str:
        """Constructs the prompt, dynamically incorporating the required tool structure."""

        required_tools = ", ".join([f"'{tool}'" for tool in self.search_tiers.keys()])

        tier_instructions = "\n".join([
            f"-   **{tool.capitalize()}**: Requires keys: {', '.join([f'`{tier}`' for tier in tiers])}"
            for tool, tiers in self.search_tiers.items()
        ])

        example_queries = {
            tool: {tier: f"<{tool}_{tier}_query>" for tier in tiers}
            for tool, tiers in self.search_tiers.items()
        }
        example_output = {
            "tiered_queries": example_queries,
            "material_elements": ["<Single_Compound_Formula_If_Applicable>", "Fe", "P", "O"] # Updated structure hint
        }

        return f"""
        You are an expert research planner. Your task is to analyze the user's research query,
        and generate mandatory, non-empty search queries for the following tools: {required_tools}.

        **Mandatory Query Requirements:**
        -   Every query field (e.g., 'strict', 'moderate', 'simple') must contain a **non-empty, effective search string**.
        -   If the query mentions a specific chemical compound (e.g., 'LiFePO4', 'LiCoO2'), the **first element** in the 'material_elements' list MUST be the **single compound formula**. Subsequent elements can be the constituent elements. If no elements are present, use an empty list [].

        **Semantic Query:** "{semantic_query}"

        **Required Output Structure:**
        The output MUST be a single JSON object (Dict) with two primary keys: **'tiered_queries'** and **'material_elements'**.

        **Tool and Tier Specification:**
        {tier_instructions}

        **Example Output Structure (MUST be strictly followed, replace <...> with actual queries/elements):**
        {json.dumps(example_output, indent=4)}
        """

    def _call_llm_and_parse(self, prompt: str) -> Optional[Dict[str, Any]]:
        """Handles the LLM call, JSON parsing, and error reporting."""
        if client is None: return None
        try:
            print(f"{C_BLUE}[{self.id} ACTION] Calling LLM ({self.model}) to generate structured queries...{C_RESET}")
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a research planning expert. Output ONLY the requested JSON object."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            json_str = response.choices[0].message.content.strip()
            llm_output = json.loads(json_str)

            print(f"{C_YELLOW}[{self.id.upper()} DEBUG] Raw LLM Output: {llm_output}{C_RESET}")

            if 'tiered_queries' not in llm_output or 'material_elements' not in llm_output:
                 raise ValueError("LLM output is missing the mandatory 'tiered_queries' or 'material_elements' key.")

            return llm_output

        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] LLM call or JSON parsing failed: {e}{C_RESET}")
            return None

    def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """The main execution method for the agent."""
        print(f"\n{C_ACTION}[{self.id.upper()} START] Generating tiered search queries and extracting elements (LLM)...{C_RESET}")

        semantic_query = state.get("semantic_query", "")
        if not semantic_query:
            print(f"{C_RED}[{self.id.upper()} FAIL] Semantic query is empty. Cannot generate queries.{C_RESET}")
            return state

        prompt = self._format_prompt(semantic_query)
        llm_output = self._call_llm_and_parse(prompt)

        if llm_output and llm_output.get("tiered_queries"):
            state["tiered_queries"] = llm_output["tiered_queries"]
            state["material_elements"] = llm_output.get("material_elements", [])
            # Set the primary search term for fallback and MaterialsAgent/SynthesisAgent targeting
            state["api_search_term"] = state["material_elements"][0] if state["material_elements"] else semantic_query

            query_dict = state["tiered_queries"]
            num_queries = sum(
                1 for tool_queries in query_dict.values()
                for query in tool_queries.values() if query.strip()
            )

            print(f"{C_YELLOW}[{self.id.upper()} STATE] Generated {num_queries} non-empty queries. Elements extracted: {state['material_elements']}{C_RESET}")
            print(f"{C_GREEN}[{self.id.upper()} DONE] Queries generated. Ready for tool execution.{C_RESET}")
        else:
            print(f"{C_RED}[{self.id.upper()} FAIL] Could not generate valid queries. Initializing fallback structure.{C_RESET}")
            state["tiered_queries"] = {
                tool: {tier: semantic_query if tier == 'simple' and tool in ['web', 'openalex'] else "" for tier in tiers}
                for tool, tiers in self.search_tiers.items()
            }
            state["material_elements"] = []

        return state


# ==================================================================================================
# SECTION 7: RETRIEVAL AGENT (OPTIMIZED: LLM-Based Smart Chunking FILTER BYPASSED) - FIXED FOR ABSTRACTS
# ==================================================================================================

class RetrievalAgent:
    def __init__(self, agent_id: str = "retrieval_agent", chunk_size: int = 500, model: str = LLM_MODEL):
        self.id = agent_id
        self.chunk_size = chunk_size
        self.model = model

    def _download_pdf(self, url: str) -> Optional[BytesIO]:
        try:
            time.sleep(1)
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            return BytesIO(response.content)
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] Failed to download PDF from {url[:30]}...: {e}{C_RESET}")
            return None

    def _extract_text_from_pdf(self, pdf_stream: BytesIO) -> str:
        try:
            reader = PdfReader(pdf_stream)
            text = ""
            for page in reader.pages:
                text += page.extract_text() or ""
            return text
        except Exception as e:
            print(f"{C_RED}[{self.id} ERROR] PDF text extraction failed: {e}{C_RESET}")
            return ""

    def _chunk_text(self, text: str) -> List[str]:
        # Simple text splitting by character count
        return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]

    def execute(self, state: ResearchState) -> ResearchState:
        print(f"\n{C_ACTION}[{self.id.upper()} START] Downloading PDFs and running **Optimized Chunking (Abstract Fallback)**...{C_RESET}")

        raw_data = state.get('raw_tool_data', [])
        pdf_entries = [d for d in raw_data if d.get('metadata', {}).get('pdf_url')]

        state.setdefault('full_text_chunks', [])
        all_new_chunks = []
        downloaded_urls = set() # Track URLs that were successfully downloaded

        # 1. --- PDF DOWNLOAD AND CHUNKING ---
        for entry in pdf_entries:
            pdf_url = entry['metadata']['pdf_url']
            pdf_stream = self._download_pdf(pdf_url)

            if not pdf_stream: continue

            text = self._extract_text_from_pdf(pdf_stream)
            if not text.strip(): continue

            downloaded_urls.add(pdf_url)
            raw_chunks = self._chunk_text(text)
            print(f"[{C_BLUE}{self.id}{C_RESET}] PDF from {pdf_url[:20]}... yielded {len(raw_chunks)} raw chunks.")

            llm_filtered_chunks = raw_chunks # Optimization: Filter is bypassed

            # Add source metadata to each chunk
            for chunk in llm_filtered_chunks:
                 all_new_chunks.append({
                     "text": chunk,
                     "source": entry['tool_id'],
                     "url": pdf_url
                 })

            print(f"[{C_GREEN}{self.id}{C_RESET}] LLM filter **SKIPPED**. {len(llm_filtered_chunks)} chunks from this document are now passed to RAG.{C_RESET}")

        # 2. --- ABSTRACT/SNIPPET FALLBACK CHUNKING (The Fix) ---

        # Process entries that did *not* have a PDF URL or whose PDF failed to download
        text_entries = [
            d for d in raw_data
            if d.get('text') # Must have text (abstract/snippet)
        ]

        abstract_chunk_count = 0

        for entry in text_entries:
            # Skip if this entry was already processed as a successful PDF download
            if entry.get('metadata', {}).get('pdf_url') in downloaded_urls:
                continue

            # Skip web searches if PDF retrieval was successful on other documents (optional optimization)
            # We keep this simple: If it has text and we haven't processed its PDF, process the text.

            raw_text = entry['text'].strip()
            source_url = entry.get('metadata', {}).get('url', 'N/A')

            if raw_text:
                # Treat the abstract/snippet as a single chunk (or chunk it if very long)
                abstract_chunks = self._chunk_text(raw_text)

                for chunk in abstract_chunks:
                     all_new_chunks.append({
                         "text": chunk,
                         "source": entry['tool_id'],
                         "url": source_url
                     })
                abstract_chunk_count += len(abstract_chunks)

        if abstract_chunk_count > 0:
            print(f"[{C_BLUE}{self.id}{C_RESET}] Extracted and chunked {abstract_chunk_count} abstracts/snippets as fallback content.")


        state['full_text_chunks'].extend(all_new_chunks)

        if not state['full_text_chunks']:
            # Fallback to the flow control message only if absolutely no data was found
             state['full_text_chunks'] = [{"text": "No full-text or abstract/snippet documents were retrieved from the search results.", "source": "flow_control", "url": ""}]
             print(f"{C_RED}[{self.id} FAIL] No content found after all attempts. Using flow control message.{C_RESET}")


        print(f"{C_YELLOW}[{self.id.upper()} STATE] Total full text chunks for RAG: {len(state['full_text_chunks'])}{C_RESET}")
        print(f"{C_GREEN}[{self.id.upper()} DONE] Full-text retrieval and **Optimized** chunking complete.{C_RESET}")

        return state


# ==================================================================================================
# SECTION 8: RAG AGENT WITH VECTOR SEARCH (FINALIZED LOGIC)
# ==================================================================================================

# Function to get embedding (requires global 'client' which is OpenAI client)
# Assumes DIMENSION, EMBED_MODEL, client, np, os, faiss, pickle, VECTOR_INDEX_PATH, and VECTOR_DATA_PATH are defined globally
def _get_embedding(text: str) -> np.ndarray:
    if client is None: return np.zeros(DIMENSION, dtype=np.float32)
    try:
        response = client.embeddings.create(input=text, model=EMBED_MODEL)
        return np.array(response.data[0].embedding, dtype=np.float32)
    except Exception as e:
        print(f"{C_RED}[EMBEDDING ERROR] Failed to get embedding: {e}{C_RESET}")
        return np.zeros(DIMENSION, dtype=np.float32)

class VectorDBWrapper:
    def __init__(self, dimension: int = DIMENSION):
        self.dimension = dimension
        self.index: faiss.Index = None
        self.text_store: List[str] = []
        if client is not None:
             self._initialize_db()
        else:
             print(f"{C_RED}[VectorDB] Skipping initialization due to missing API key.{C_RESET}")

    def _initialize_db(self):
        if os.path.exists(VECTOR_INDEX_PATH) and os.path.exists(VECTOR_DATA_PATH):
            try:
                self.index = faiss.read_index(VECTOR_INDEX_PATH)
                with open(VECTOR_DATA_PATH, "rb") as f:
                    self.text_store = pickle.load(f)
                print(f"{C_CYAN}[VectorDB] Loaded existing DB. Chunks: {len(self.text_store)}{C_RESET}")
            except Exception:
                print(f"{C_RED}[VectorDB] Failed to load existing DB. Creating new one.{C_RESET}")
                self._create_new_db()
        else:
            self._create_new_db()

    def reset_db(self):
        """Resets the in-memory index and text store and clears the persistent files."""
        print(f"{C_RED}[VectorDB] Starting database reset for new session...{C_RESET}")

        # 1. Reset the FAISS index to an empty state
        self.index = faiss.IndexFlatL2(self.dimension)

        # 2. Clear the in-memory text store
        self.text_store = []

        # 3. Optionally delete persistent files (to save the empty state)
        if os.path.exists(VECTOR_INDEX_PATH):
            os.remove(VECTOR_INDEX_PATH)
        if os.path.exists(VECTOR_DATA_PATH):
            os.remove(VECTOR_DATA_PATH)

        # 4. Save the empty state to disk
        self._save_db()
        print(f"{C_GREEN}[VectorDB] Database reset complete.{C_RESET}")

    def _create_new_db(self):
        self.index = faiss.IndexFlatL2(self.dimension)
        self.text_store = []
        self._save_db()
        print(f"{C_CYAN}[VectorDB] Created new IndexFlatL2 DB.{C_RESET}")

    def _save_db(self):
        faiss.write_index(self.index, VECTOR_INDEX_PATH)
        with open(VECTOR_DATA_PATH, "wb") as f:
            pickle.dump(self.text_store, f)

    def add_chunks(self, chunks: List[str]):
        if client is None: return
        new_embeddings_list = []
        for chunk in chunks:
            if chunk not in self.text_store and chunk.strip():
                embedding = _get_embedding(chunk)
                if not np.all(embedding == 0):
                    new_embeddings_list.append(embedding)
                    self.text_store.append(chunk)
        if new_embeddings_list:
            new_embeddings = np.array(new_embeddings_list).astype('float32')
            self.index.add(new_embeddings)
            self._save_db()
            # Note: The log in the run might still show 'Added 1 new chunks' if it's the result of this function call,
            # even if it was called with a list of 5 (due to the inner loop count).
            print(f"{C_BLUE}[VectorDB] Added {len(new_embeddings_list)} new chunks.{C_RESET}")

    def search(self, query: str, k: int = 20) -> List[Tuple[str, float]]:
        if client is None or self.index is None: return []
        if self.index.ntotal == 0: return []
        query_embedding = _get_embedding(query).reshape(1, -1)
        if np.all(query_embedding == 0):
             print(f"{C_RED}[VectorDB ERROR] Search failed due to invalid query embedding.{C_RESET}")
             return []

        D, I = self.index.search(query_embedding, k)
        results: List[Tuple[str, float]] = []
        for dist, idx in zip(D[0], I[0]):
            if idx != -1:
                results.append((self.text_store[idx], dist))
        return results

class RAGAgent:
    def __init__(self, agent_id: str = "rag_agent", distance_threshold: float = 1.3, max_chunks_to_keep: int = 5, vector_db: Optional[VectorDBWrapper] = None):
        self.id = agent_id
        # Base/Strict Parameters
        self.base_distance_threshold = distance_threshold
        self.max_chunks_to_keep = max_chunks_to_keep
        self.vector_db = vector_db if vector_db is not None else VectorDBWrapper()

    def _passes_keyword_gate(self, chunk: str, literal_term: str) -> bool:
        chunk_lower = chunk.lower()
        is_academic_noise = ("full text" in chunk_lower or "arxiv paper" in chunk_lower or "pubmed abstract" in chunk_lower)
        contains_literal = literal_term.lower() in chunk_lower if literal_term else True
        return not (is_academic_noise and not contains_literal)

    def execute(self, state: ResearchState) -> ResearchState:
        print(f"\n{C_ACTION}[{self.id.upper()} START] Running RAG: Vector Search and Dual-Filter Guardrail...{C_RESET}")

        if client is None or self.vector_db.index is None:
             state['filtered_context'] = "RAG processing skipped due to missing API Key or Vector DB initialization failure."
             print(f"{C_RED}[{self.id} FAIL] RAG processing skipped.{C_RESET}")
             state['rag_complete'] = True
             return state

        query = state.get('semantic_query', '')
        literal_term = state.get("api_search_term", "").lower()
        raw_data = state.get('raw_tool_data', [])

        # --- 1. Separate Data Streams (Ensures Web Snippets are indexed) ---
        structured_chunks: List[dict] = state.get('full_text_chunks', [])
        chunks_for_db: List[str] = [
            c.get('text') for c in structured_chunks
            if isinstance(c, dict) and c.get('text')
        ]

        structured_context_list = []
        for d in raw_data:
            if d.get('tool_id') == 'materials_agent' and d.get('text'):
                structured_context_list.append(f"--- Structured Data (Materials Property) ---\n{d['text']}")
            elif not chunks_for_db and d.get('text') and d.get('tool_id') == 'web_agent':
                 # CRITICAL: If no PDF full-text chunks, treat raw web snippets as chunks for indexing.
                 chunks_for_db.append(d['text'])

        # Check if the Web Agent was the primary tool and if so, relax the overall filter.
        is_general_query = 'web' in state.get('active_tools', []) and len(state.get('active_tools', [])) == 1

        if not chunks_for_db and not structured_context_list:
             state['filtered_context'] = "No external data was retrieved or processed."
             print(f"{C_RED}[{self.id} FAIL] No data available for RAG processing.{C_RESET}")
             state['rag_complete'] = True
             return state

        # 2. Dynamic Parameter Adjustment (Refinement Logic)
        is_refining = state.get("is_refining", False)

        # ðŸŸ¢ FIX: Set a more appropriate base threshold for RAG, especially for general/web content
        current_threshold = 1.6 if is_general_query else 1.45
        current_query = query

        if is_refining:
            current_threshold = 2.0 # <--- HIGHLY RELAXED FOR FAILED REFINEMENT
            refine_reason = state.get('refinement_reason', '')
            if refine_reason:
                 current_query = refine_reason
                 print(f"{C_BLUE}[{self.id} INFO] Refinement Mode Active: Query set to REASON ('{current_query[:50]}...').{C_RESET}")

            print(f"{C_YELLOW}[{self.id} INFO] Refinement Mode Active: Relaxing distance threshold to {current_threshold:.2f}.{C_RESET}")
        else:
             print(f"{C_BLUE}[{self.id} INFO] Normal Mode: Using relaxed distance threshold of {current_threshold:.2f} (Base={is_general_query}).{C_RESET}")


        # 3. Add/Update Vector DB (using content from PDFs, Abstracts, or Web Snippets)
        self.vector_db.add_chunks(chunks_for_db)
        print(f"{C_BLUE}[{self.id} INFO] Vector DB contains {self.vector_db.index.ntotal} total chunks.{C_RESET}")

        # 4. Search and Filter (Vector Search)
        top_k_results: List[Tuple[str, float]] = self.vector_db.search(current_query, k=10)
        final_filtered_chunks: List[str] = []

        # ðŸŸ¢ Secondary Search Check
        if not top_k_results and self.vector_db.index.ntotal > 0:
            print(f"{C_YELLOW}[{self.id} WARN] Initial vector search failed. Running broader search against all chunks.{C_RESET}")
            current_threshold = max(current_threshold, 1.8) # Temporarily relax to 1.8 if not already 2.0
            top_k_results = self.vector_db.search(current_query, k=15)

        if not top_k_results:
             # This means FAISS returned no results at all.
             print(f"{C_RED}[{self.id} CRITICAL FAISS FAILURE] FAISS returned no results. Bypassing search and including all {len(chunks_for_db)} raw snippets.{C_RESET}")
             final_filtered_chunks = chunks_for_db
        else:
            # Normal filtering logic
            for rank, (chunk, distance) in enumerate(top_k_results):
                if distance > current_threshold:
                     # Log the skip to diagnose if filtering is too aggressive
                     print(f"{C_YELLOW}[{self.id} DEBUG] Skipping chunk (Dist={distance:.2f} > Threshold={current_threshold:.2f}).{C_RESET}")
                     continue
                if not self._passes_keyword_gate(chunk, literal_term): continue

                final_filtered_chunks.append(chunk)
                if len(final_filtered_chunks) >= self.max_chunks_to_keep: break

        # ðŸŸ¢ FINAL CRITICAL FIX (New logic): If the normal filtering yielded nothing, use the raw chunks.
        if not final_filtered_chunks and chunks_for_db:
             print(f"{C_RED}[{self.id} FALLBACK: ZERO HITS] All search results filtered by threshold. Including {len(chunks_for_db)} raw snippets as context.{C_RESET}")
             final_filtered_chunks = chunks_for_db


        # 5. Combine Contexts
        final_context_list = structured_context_list + final_filtered_chunks

        # ðŸŸ¢ FIX: If context is still empty after RAG, put a message in the log, but allow Synthesis to try
        if not final_context_list:
            final_context = "No sufficiently relevant context found."
        else:
            final_context = "\n---\n".join(final_context_list)

        state['filtered_context'] = final_context

        # 6. Final State Update
        state['is_refining'] = False
        state['rag_complete'] = True

        print(f"{C_YELLOW}[{self.id.upper()} STATE] Final context generated from **{len(final_context_list)}** chunks (including {len(structured_context_list)} structured).{C_RESET}")
        print(f"{C_GREEN}[{self.id.upper()} DONE] RAG filtering complete.{C_RESET}")
        return state


# ==================================================================================================
# SECTION 9: SYNTHESIS AGENT (LLM-Driven) - FIXED FOR DYNAMIC STRUCTURE AND REFINEMENT & RELEVANCE
# ==================================================================================================
import re

class SynthesisAgent:
    def __init__(self, agent_id: str = "synthesis_agent", model: str = LLM_MODEL):
        self.id = agent_id
        self.model = model

    def _extract_material_data(self, state: ResearchState) -> tuple[str, str, bool]:
        """
        Dynamically extracts and formats specific material properties from raw_tool_data.
        Returns: (data_summary, target_formula, data_is_present)
        """
        # CRITICAL FIX: Base target formula on the state, do NOT use a hardcoded fallback like 'LiFePO4'
        target_formula = state.get('material_formula', state.get('api_search_term', 'N/A'))

        materials_results = [
            d for d in state.get("raw_tool_data", [])
            if d.get("tool_id") == "materials_agent"
        ]

        material_data = []

        # Find data specifically formatted by the materials agent
        for result in materials_results:
            # We are less strict here as long as the materials agent returned *something*
            material_data.append(result.get('text', 'N/A'))
            break # Take only the first relevant material result

        data_is_present = bool(material_data)

        if data_is_present:
            # Return the structured text
            return "\n".join(material_data), target_formula, data_is_present
        else:
            # Return a simple fallback for the prompt
            return f"No material property data was retrieved for {target_formula}.", target_formula, data_is_present


    # --- Reference Extraction (Keep Unchanged) ---
    def _extract_references(self, state: ResearchState) -> str:
        """
        Formats the list of references gathered throughout the workflow,
        converting article titles and web snippets into clickable Markdown links
        by cross-referencing against the raw_tool_data URLs.
        """
        references = state.get("references", [])
        raw_data = state.get("raw_tool_data", [])

        # 1. Prepare a URL lookup table: Map a reference's text snippet to its URL
        url_lookup = {}

        for entry in raw_data:
            metadata = entry.get('metadata', {})
            source_type = entry.get('source_type')

            url = None
            ref_snippet = None

            if source_type == 'web_search' and metadata.get('url'):
                url = metadata['url']
                # The ref_snippet must match the start of the string generated in state['references']
                ref_snippet = f"ðŸ”— Web Source: {metadata.get('title')}"

            elif source_type == 'arxiv' and metadata.get('pdf_url'):
                url = metadata['pdf_url']
                ref_snippet = f"ðŸ”— Arxiv: {metadata.get('title')}"

            elif source_type in ['pubmed', 'openalex']:
                # Prioritize a direct PDF/external URL
                if metadata.get('pdf_url'):
                     url = metadata['pdf_url']
                elif metadata.get('external_id'): # Use PMID/DOI to construct a PubMed link
                     url = f"https://pubmed.ncbi.nlm.nih.gov/{metadata['external_id']}/"

                if url:
                    # Capture the title part for the lookup key
                    ref_snippet = f"ðŸ“„ Journal Article: {metadata.get('title')}"

            if url and ref_snippet:
                # Store the key part of the reference string for later lookup
                url_lookup[ref_snippet.strip()] = url.strip()


        # 2. Process and Format References
        unique_references = sorted(list(set(references)))
        formatted_list = []

        mp_pattern = re.compile(r'(âš›ï¸ Materials Project: [^\(]+)\s+\(([^\)]+)\)')
        web_url_pattern = re.compile(r'\((https?://[^\)]+)\)')
        lookup_prefixes = ('ðŸ“„ Journal Article:', 'ðŸ”— Arxiv:')

        for i, ref in enumerate(unique_references, 1):
            ref_stripped = ref.strip()
            markdown_link = ""
            url_found_and_formatted = False

            if ref_stripped.startswith(lookup_prefixes):
                for snippet, url in url_lookup.items():
                    if ref_stripped.startswith(snippet):
                        markdown_link = f"[{i}] [{ref_stripped}]({url})"
                        url_found_and_formatted = True
                        break

            if not url_found_and_formatted:
                web_match = web_url_pattern.search(ref_stripped)
                if web_match:
                    url = web_match.group(1)
                    display_text = ref_stripped[:web_match.start()].strip()
                    markdown_link = f"[{i}] [{display_text}]({url})"
                    url_found_and_formatted = True

            if not url_found_and_formatted:
                mp_match = mp_pattern.match(ref_stripped)
                if mp_match:
                    markdown_link = f"[{i}] {ref_stripped}"
                else:
                    markdown_link = f"[{i}] {ref_stripped}"

            formatted_list.append(markdown_link)

        return "\n".join(formatted_list)
    # ----------------------------------------

    # ðŸŸ¢ NEW: CONTEXT RELEVANCE GUARDRAIL
    def _check_context_relevance(self, query: str, context: str) -> bool:
        """Uses the LLM to verify if the minimal context is topically relevant to the query."""
        if client is None: return True # Cannot check, assume relevant

        relevance_prompt = f"""
        Analyze the following context snippet and determine if it contains relevant information
        to answer the user's core question.

        Core Question: "{query}"

        Context Snippet: "{context[:500]}..."

        Respond ONLY with the single word 'YES' or 'NO'.
        """

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": relevance_prompt}],
                temperature=0.0,
                max_tokens=5
            )
            llm_response = response.choices[0].message.content.strip().upper()

            if "YES" in llm_response:
                print(f"{C_GREEN}[SYNTHESIS GUARDRAIL] Context passed relevance check (YES).{C_RESET}")
                return True
            else:
                print(f"{C_RED}[SYNTHESIS GUARDRAIL] Context failed relevance check (NO/Irrelevant).{C_RESET}")
                return False
        except Exception as e:
            print(f"{C_RED}[SYNTHESIS GUARDRAIL ERROR] LLM check failed: {e}. Assuming relevant to proceed.{C_RESET}")
            return True # Default to proceeding to avoid crashing the workflow


    def _format_prompt(self, state: ResearchState) -> str:
        # --- Common Data Extraction ---
        query = state.get("semantic_query", "No query provided")
        rag_context = state.get("filtered_context", "No context available")
        execution_plan = "\n- ".join(state.get("execution_plan", ["No plan available"]))
        formatted_references = self._extract_references(state)
        material_data_summary, target_formula, data_is_present = self._extract_material_data(state)

        # --- DYNAMIC STRUCTURE LOGIC ---
        if data_is_present:
            # Materials-focused report structure
            first_section_heading = f"## Stability and Bandgap of {target_formula}"
            first_section_data_name = "**A. FILTERED MATERIAL PROPERTIES (Stability and Bandgap):**"
            first_section_content_req = f"""
                    * The first section must use **Data Source A** to describe the stability and bandgap, ideally presented in a **Markdown Table**.
                    * The second section must summarize findings from **Data Source B** relevant to the overall query.
            """
        else:
            # Literature Review-focused report structure (Default for non-material queries)
            first_section_heading = "## Introduction and Scope of Review"
            first_section_data_name = "**A. FILTERED MATERIAL PROPERTIES (N/A):**"
            first_section_content_req = f"""
                    * The first section must provide a general overview and define the scope of the review based on **Data Source B**.
                    * The subsequent sections must summarize the key findings from **Data Source B** (e.g., categorizing by cancer type, outcome, or experimental method).
            """

        final_structure_headings = f"""
                3.  **Structure:** The final report **MUST** follow this exact **four-section** structure using Level 2 Markdown headings (##):
                    * {first_section_heading}
                    * ## Key Research Findings
                    * ## Conclusion and Future Outlook
                    * ## References
        """

        # ==================================================================================
        # ðŸŸ¢ CRITICAL REFINEMENT PROMPT LOGIC
        # ==================================================================================
        if state['needs_refinement']:
            refinement_reason = state.get('refinement_reason', 'The previous report was incomplete or inaccurate.')
            previous_report = state.get('final_report', 'Previous report text is unavailable.')

            return f"""
                **AGENT ROLE**: You are a dedicated **Scientific Report REFINEMENT EXPERT**. Your sole function is to rewrite the previous report to address the critical feedback provided below. You must produce a single, cohesive, final report.

                ---
                **I. REFINEMENT MANDATE**
                **CRITICAL FEEDBACK:** {refinement_reason}

                **PREVIOUS REPORT (To be Rewritten):**
                {previous_report}
                ---

                **II. DATA SOURCES (For Context and Grounding)**

                {first_section_data_name}
                {material_data_summary}

                **B. FILTERED RESEARCH CONTEXT (Recent Articles for Synthesis):**
                {rag_context}

                **C. RAW REFERENCE LIST (For Final Output):**
                {formatted_references}

                ---
                **III. REPORT REWRITE INSTRUCTIONS**

                1.  **Primary Goal:** **CRITICALLY ADDRESS THE FEEDBACK** in Section I. If the issue was irrelevant links (as in your test case), you must either ensure the links in the references are relevant, or that the discussion in the report is properly grounded by the relevant sources.
                2.  **Strict Adherence:** Rewrite the report using **only** the data provided in Section II.
                3.  **Scientific Tone & Citation (CRITICAL):** The report must maintain a formal, scientific tone, and **every factual claim must be attributed** using inline numerical citations (e.g., "...the bandgap was determined to be 2.1 eV [1, 5]."). Use the index from the RAW REFERENCE LIST.
                4.  **Structure:** **Maintain the exact four-section structure** defined below:
                    {final_structure_headings}
                5.  **Final Output:** Your response must **ONLY** contain the final rewritten report.
                """
        # ==================================================================================
        # ðŸŸ¢ INITIAL REPORT PROMPT LOGIC (Original path)
        # ==================================================================================
        else:
            return f"""
                **AGENT ROLE**: You are a dedicated **Scientific Research Assistant**. Your sole function is to compile a final, comprehensive, and objective research report that directly addresses the user's query using only the provided filtered data.

                ---
                **I. RESEARCH MANDATE & PLAN**
                **User Query:** {query}
                **Execution Plan:**
                - {execution_plan}
                ---

                **II. DATA SOURCES**

                {first_section_data_name}
                {material_data_summary}

                **B. FILTERED RESEARCH CONTEXT (Recent Articles for Synthesis):**
                {rag_context}

                **C. RAW REFERENCE LIST (For Final Output):**
                {formatted_references}

                ---
                **III. REPORT GENERATION INSTRUCTIONS**

                1.  **Strict Adherence:** Generate the report using **only** the data provided in Section II. Do not hallucinate or use external knowledge.
                2.  **Scientific Tone & Citation (CRITICAL):** The entire report must be written in a formal, **scientific, and objective tone** (avoiding personal pronouns or conversational language). **Every factual claim and data point must be attributed** using inline numerical citations (e.g., "...the bandgap was determined to be 2.1 eV [1, 5]."). Use the index from the RAW REFERENCE LIST.
                3.  **Structure:** {final_structure_headings}
                4.  **Content Requirements:**
                    {first_section_content_req}
                5.  **Final Output:** The final section, **## References**, must be a direct copy of the list provided in **Data Source C**. Your response must **ONLY** contain the final report.
                """

    def execute(self, state: ResearchState) -> ResearchState:
        # ðŸš¨ Refinement Check: Mark the start of the refinement or initial generation
        mode = "REFINEMENT" if state.get('needs_refinement') else "INITIAL GENERATION"
        print(f"\n{C_ACTION}[{self.id.upper()} START] Running {mode} (LLM: {self.model})...{C_RESET}")

        # Check for initial errors
        if client is None:
             state['final_report'] = "Synthesis failed: LLM is not initialized due to missing API Key."
             print(f"{C_RED}[{self.id} FAIL] Synthesis skipped due to missing API Key.{C_RESET}")
             return state

        context = state.get("filtered_context", "")
        query = state.get("semantic_query", "")
        is_refining = state.get('needs_refinement', False)

        # ðŸŸ¢ NEW: CONTEXT RELEVANCE GUARDRAIL (Anti-GIGO check)
        # Check if context is short AND it's the initial run
        if not is_refining and (len(context) < 200 or context.startswith("No sufficiently relevant context")):
            print(f"{C_YELLOW}[SYNTHESIS GUARDRAIL] Minimal context detected (Length: {len(context)}). Running relevance check...{C_RESET}")

            if not self._check_context_relevance(query, context):
                # Graceful Failure State: Irrelevant context detected
                state['final_report'] = f"The initial search yielded data highly irrelevant to the query '{query}'. The system cannot generate a meaningful report based on the provided context."
                state['report_generated'] = True # Mark as generated to allow Evaluation to read the failure message
                state['needs_refinement'] = False
                print(f"{C_RED}[{self.id} FAIL] Graceful failure: Irrelevant context detected. Outputting failure message.{C_RESET}")
                return state

        # Original check for true starvation (after potential relevance failure has passed)
        if context.strip() in ["No sufficiently relevant context found.", ""]:
             state['final_report'] = "Research failed: Could not find sufficient relevant data to generate a report for the query: " + state.get("user_query", "N/A")
             print(f"{C_RED}[{self.id} FAIL] Synthesis skipped due to lack of context.{C_RESET}")
             return state

        # Generate the appropriate prompt (initial or refinement)
        prompt = self._format_prompt(state)

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": "You are a scientific research assistant who outputs a final, structured report with inline citations and a reference list."},
                          {"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=3000 # Increased max tokens for refinement rewrite safety
            )
            final_text = response.choices[0].message.content.strip()
            state['final_report'] = final_text

            # --- CRITICAL REFINEMENT FLAGS UPDATE ---
            state['report_generated'] = True
            state['is_refining'] = state['needs_refinement'] # Track if this run was a rewrite
            state['needs_refinement'] = False # Reset the flag for the next evaluation run
            state['next'] = 'evaluate_node' # Route back to evaluation to check the rewrite
            # ----------------------------------------

            print(f"{C_YELLOW}[{self.id.upper()} STATE] Final report generated (Approx. {len(final_text.split())} words).{C_RESET}")
            print(f"{C_GREEN}[{self.id.upper()} DONE] Report generation successful. Next: Evaluate.{C_RESET}")

        except Exception as e:
            state['final_report'] = f"Error: Unable to generate report during {mode} due to LLM failure or API issue."
            state['report_generated'] = False
            state['next'] = 'TERMINATE'
            print(f"{C_RED}[{self.id} ERROR] Failed to generate final report: {e}{C_RESET}")

        return state


# ==================================================================================================
# SECTION 9.A.: EVOLUTION AGENT
# ==================================================================================================
class EvaluationSchema(BaseModel):
    """Schema for Evaluation Agent output to ensure reliable boolean routing."""
    needs_refinement: bool = Field(description="Set to TRUE if the final_report fails to address a key, actionable part of the execution plan. Otherwise, set to FALSE.")
    refinement_reason: str = Field(description="Specific reason why refinement is needed (e.g., 'Missing data on performance degradation'), or 'Report is satisfactory' if FALSE.")

class EvaluationAgent:
    """
    Evaluates the Synthesis Agent's report against the dynamic execution plan.
    Uses GPT-4 for reliable decision-making via structured Pydantic output.
    """
    def __init__(self, agent_id: str = "evaluation_agent"):
        self.id = agent_id

        # 1. Retrieve the custom API key from the environment
        custom_key = os.getenv("GPT_API_KEY")

        if not custom_key:
            raise EnvironmentError("GPT_API_KEY is not set. Cannot initialize EvaluationAgent.")

        # 2. Initialize ChatOpenAI client and attach structured output schema
        # CRITICAL FIX: Use .with_structured_output to configure JSON output correctly.
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-2024-04-09", # Use a reliable model for JSON output
            temperature=0.0,
            api_key=custom_key
        ).with_structured_output(EvaluationSchema)

        print(f"[{self.id.upper()} INIT] Initialized with model gpt-4-turbo-2024-04-09 using structured output.{C_RESET}")


    def run(self, state: ResearchState) -> ResearchState:
        print(f"\n{C_ACTION}[{self.id.upper()} START] Evaluating synthesized report against execution plan...{C_RESET}")

        user_query = state['user_query']
        execution_plan = state['execution_plan']
        final_report = state['final_report']

        eval_prompt = f"""
        You are a highly critical Research Evaluation Agent. Your purpose is to determine if the
        'Final Report' successfully addresses all steps and requirements laid out in the 'Execution Plan'.

        --- INPUTS ---
        Original User Query: {user_query}
        Execution Plan (Mandatory Tasks): {execution_plan}
        Synthesized Final Report: {final_report}

        --- CRITERIA ---
        1. Completeness: Does the report contain evidence or a summary for every major step outlined in the Execution Plan?
        2. Relevance: Does the report stay focused on the user query?

        Based on your critique, generate the required structured output.
        """

        try:
            # CRITICAL FIX: Call invoke without the 'response_schema' argument.
            # The structured output is now configured on the self.llm object itself.
            response: EvaluationSchema = self.llm.invoke(eval_prompt)

            # Update state with the clean, reliable boolean output
            state['needs_refinement'] = response.needs_refinement
            state['refinement_reason'] = response.refinement_reason
            state['report_generated'] = True

            # --- Print Decision and Post-Evaluation State ---
            print(f"{C_MAGENTA}[{self.id.upper()} RESULT] Needs Refinement: {response.needs_refinement}{C_RESET}")
            print(f"{C_MAGENTA}[{self.id.upper()} REASON] {response.refinement_reason}{C_RESET}")
            print(f"{C_BLUE}[{self.id.upper()} DEBUG] Post-Evaluation Flags Set: report_generated=True, needs_refinement={response.needs_refinement}{C_RESET}")

            print(f"{C_MAGENTA}[{self.id.upper()} RESULT] Needs Refinement: {response.needs_refinement}. Reason: {response.refinement_reason[:50]}...{C_RESET}")

        except Exception as e:
            # Note: This fallback will now only trigger on truly unexpected errors (e.g., network failure, rate limiting).
            print(f"{C_RED}[{self.id} ERROR] LLM Evaluation failed unexpectedly: {e}. Defaulting to completion to prevent infinite loop.{C_RESET}")
            state['needs_refinement'] = False
            state['refinement_reason'] = "Evaluation failed due to API/network error. Assuming report is complete."

        return state


# ==================================================================================================
# SECTION 10: LANGGRAPH ASSEMBLY AND ORCHESTRATION (SUPERVISOR-CONTROLLED DYNAMIC FLOW)
# ==================================================================================================

# --- AGENT INSTANTIATION & SETUP (Final Corrected List) ---
db_wrapper = VectorDBWrapper()

supervisor_agent = SupervisorAgent()
clean_query_agent = CleanQueryAgent()
intent_agent = IntentAgent(model=LLM_MODEL)
planning_agent = PlanningAgent(model=LLM_MODEL)
query_generation_agent = QueryGenerationAgent(model=LLM_MODEL)
retrieval_agent = RetrievalAgent(model=LLM_MODEL)
synthesis_agent = SynthesisAgent(model=LLM_MODEL)
web_agent = WebAgent()
materials_agent = MaterialsAgent()
rag_agent = RAGAgent(vector_db=db_wrapper)
evaluation_agent = EvaluationAgent() # Use the new agent
pubmed_agent = PubMedAgent()
arxiv_agent = ArxivAgent()
openalex_agent = OpenAlexAgent()

def evaluation_node(state: ResearchState) -> ResearchState:
    return evaluation_agent.run(state)


# Helper function wrappers for the agents
def clean_query_node(state: ResearchState) -> ResearchState: return clean_query_agent.run(state)
def intent_node(state: ResearchState) -> ResearchState: return intent_agent.run(state)
def planning_node(state: ResearchState) -> ResearchState: return planning_agent.run(state)
def query_gen_node(state: ResearchState) -> ResearchState: return query_generation_agent.run(state)

# --- AGENT EXECUTION MAP (Corrected Methods) ---
AGENT_MAP = {
    "clean_query_node": clean_query_node,
    "intent_node": intent_node,
    "planning_node": planning_node,
    "query_gen_node": query_gen_node,

    "supervisor": supervisor_placeholder,

    "pubmed_node": pubmed_agent.execute,
    "arxiv_node": arxiv_agent.execute,
    "openalex_node": openalex_agent.execute,
    "web_node": web_agent.execute,
    "materials_node": materials_agent.execute,
    "retrieval_node": retrieval_agent.execute,
    "rag_node": rag_agent.execute,
    "synthesis_node": synthesis_agent.execute,

    "evaluation_node": evaluation_node, # <--- NEW MAPPING
}

# --- SUPERVISOR ROUTER FUNCTION (Uses updated logic from previous response) ---
def supervisor_router(state: ResearchState) -> str:
    """The main routing function. Calls the SupervisorAgent's logic to determine the next step."""
    # The supervisor_agent.select_next_agent function now contains all the logic
    # for the Evaluation/Refinement loop (checking needs_refinement)

    # We call the select_next_agent method which handles the termination and refinement logic
    next_agent_or_end = supervisor_agent.select_next_agent(state)

    if next_agent_or_end is None:
        print(f"{C_MAGENTA}[SUPERVISOR] Workflow complete. Routing to END.{C_RESET}")
        return "__end__"

    node_name = next_agent_or_end.replace("_agent", "_node")
    print(f"{C_MAGENTA}[SUPERVISOR] Routing decision: **{node_name}**{C_RESET}")
    return node_name

# --- GRAPH ASSEMBLY (Dynamic Supervisor Flow - CRITICAL UPDATES HERE) ---
workflow = StateGraph(ResearchState)

# 1. Add all functional nodes (Agents)
for node_name, node_function in AGENT_MAP.items():
    workflow.add_node(node_name, node_function)

# 2. Add the Supervisor Agent's router as a conditional edge
workflow.add_conditional_edges(
    "supervisor",
    supervisor_router,
    {
        "clean_query_node": "clean_query_node",
        "intent_node": "intent_node",
        "planning_node": "planning_node",
        "query_gen_node": "query_gen_node",
        "pubmed_node": "pubmed_node",
        "arxiv_node": "arxiv_node",
        "openalex_node": "openalex_node",
        "web_node": "web_node",
        "materials_node": "materials_node",
        "retrieval_node": "retrieval_node",
        "rag_node": "rag_node",
        "synthesis_node": "synthesis_node",
        "__end__": END,
    }
)

# 3. Set Entry Point
workflow.set_entry_point("clean_query_node")

# 4. Sequential Initial Steps
workflow.add_edge("clean_query_node", "intent_node")
workflow.add_edge("intent_node", "planning_node")
workflow.add_edge("planning_node", "query_gen_node")
workflow.add_edge("query_gen_node", "supervisor")

# 5. Connect all Data/Retrieval/RAG Agents back to the Supervisor
# Synthesis and Evaluation must be handled separately now.
for agent_node in [
    "pubmed_node", "arxiv_node", "openalex_node", "web_node", "materials_node",
    "retrieval_node", "rag_node"
]:
    workflow.add_edge(agent_node, "supervisor")

# --- NEW EVALUATION LOOP CONNECTIONS ---

# 6. Synthesis must always be followed by the Evaluation check
workflow.add_edge("synthesis_node", "evaluation_node")

# 7. Evaluation always returns to the Supervisor for the final decision
# The Supervisor's logic handles routing to __end__ or back to rag_node.
workflow.add_edge("evaluation_node", "supervisor")

# Compile the Graph
research_agent_app = workflow.compile()
print(f"{C_CYAN}[GRAPH INIT] Research Agent Dynamic Supervisor Graph compiled successfully with Evaluation Loop.{C_RESET}")


# ==================================================================================================
# SECTION 11: VISUALIZATION UTILITY
# ==================================================================================================
def visualize_graph(app, initial_query: str, filename="research_agent_academic") -> bytes:
    try:
        g = app.get_graph(xray=True)
        dot = Digraph(comment="Research Agent Academic Flow", format="png")
        dot.attr(rankdir="TB", splines="curved", nodesep="0.4", ranksep="0.8", fontname="Arial", fontsize="12")

        query_label = f"User Query:\n{initial_query[:80]}..." if len(initial_query) > 80 else f"User Query:\n{initial_query}"
        dot.node("input_query", label=query_label, shape="box3d", fillcolor="#BBDEFB", style="filled,rounded", color="#1976D2")

        dot.edge("input_query", "clean_query_node", label="Input", color="#1976D2")

        for n in g.nodes:
            node_id = getattr(n, "id", str(n))
            attrs = {"style": "filled,rounded", "fontname": "Arial", "fontsize": "12", "penwidth": "1.5"}

            llm_nodes = [
                "intent_node",
                "planning_node",
                "query_gen_node",
                "retrieval_node",
                "synthesis_node"
            ]

            if node_id == "__start__":
                continue
            elif node_id == "__end__":
                attrs.update({"shape": "doublecircle", "fillcolor": "#F44336", "fontcolor": "white", "label": "End"})
            elif node_id == "supervisor":
                attrs.update({"shape": "doubleoctagon", "fillcolor": "#FFC107", "color": "#FF9800", "label": "Supervisor"})
            elif node_id in llm_nodes:
                agent_name = node_id.replace('_node', '').title()
                if node_id == "retrieval_node":
                    agent_name = "Retrieval\n(LLM Filter)"
                elif node_id == "synthesis_node":
                     agent_name = "Synthesis\n(LLM Report)"

                attrs.update({"shape": "diamond", "fillcolor": "#90CAF9", "color": "#1976D2", "label": f"{agent_name}\n(LLM)"})

            elif node_id in ["rag_node"]:
                attrs.update({"shape": "box", "fillcolor": "#E3F2FD", "color": "#2196F3", "label": "RAG\n(Vector Search)"})
            elif node_id in ["clean_query_node"]:
                attrs.update({"shape": "box", "fillcolor": "#CFD8DC", "color": "#607D8B", "label": "Clean Query"})
            elif node_id in ["pubmed_node", "arxiv_node", "openalex_node", "web_node", "materials_node"]:
                 agent_name = node_id.replace('_node', '').title()
                 attrs.update({"shape": "box", "fillcolor": "#C8E6C9", "color": "#4CAF50", "label": f"{agent_name} Tool"})

            elif node_id == "supervisor_placeholder":
                attrs.update({"label": "Supervisor Node", "fillcolor": "#EEEEEE", "shape": "box", "style": "filled"})

            dot.node(node_id, **attrs)

        for e in g.edges:
            if e.source == "supervisor" and e.data is not None:
                label = e.data.get('select')
                dot.edge(e.source, e.target, label=label, color="#FF9800", fontcolor="#FF9800", arrowhead="normal")
            elif e.source != "supervisor" and e.source != "__start__":
                dot.edge(e.source, e.target, color="#607D8B")

        image_bytes = dot.pipe(format='png')
        return image_bytes
    except Exception as e:
        print(f"{C_RED}[VISUALIZATION ERROR] Failed to generate graph image: {e}{C_RESET}")
        return b""


# ==================================================================================================
# SECTION 12: EXECUTION BLOCK
# ==================================================================================================
if __name__ == "__main__":
    # This block is for command-line debugging/testing only and will not run when imported.

    # 1. Setup Query
    COMPLEX_QUERY = "What is the stability and bandgap of LiFePO4, and summarize two recent articles on its performance degradation mechanisms in fast-charging batteries?"

    try:
        # 2. Visualization Utility (Safe to run here)
        image_bytes = visualize_graph(research_agent_app, COMPLEX_QUERY)
        with open("research_agent_flow_fixed.png", "wb") as f:
             f.write(image_bytes)
        print(f"{C_CYAN} >> [VISUALIZATION] Graph saved to research_agent_flow_fixed.png (includes User Query node){C_RESET}")
        #
    except Exception:
        print(f"{C_RED} >> [VISUALIZATION WARNING] Graphviz not installed or error in visualization. Skipping.{C_RESET}")

    # 3. Setup Initial State (CRITICAL UPDATE HERE)
    # The initial state must now include the new flags for the Evaluation Loop.
    initial_state: ResearchState = {
        "user_query": COMPLEX_QUERY,
        "semantic_query": "",
        "primary_intent": "",
        "execution_plan": [],
        "material_elements": [],
        "api_search_term": "",
        "tiered_queries": {},
        "raw_tool_data": [],
        "full_text_chunks": [],
        "filtered_context": "",
        "references": [],
        "final_report": "",
        "rag_complete": False,
        "report_generated": False,

        # --- NEW FIELDS FOR EVALUATION LOOP (MUST BE INCLUDED) ---
        "needs_refinement": True, # Start as True to ensure the initial synthesis is always evaluated
        "refinement_reason": "",
        # --------------------------------------------------------

        "next": ""
    }

    print(f"\n{C_CYAN}================================================================={C_RESET}")
    print(f"{C_CYAN}>> STARTING RESEARCH WORKFLOW (Debug Version 7.7.4 - Stable RC) <<{C_RESET}")
    print(f"{C_CYAN}>> Query: '{COMPLEX_QUERY}'{C_RESET}")
    print(f"{C_CYAN}================================================================={C_RESET}")

    try:
        # 4. Invoke the Graph with the Recursion Limit Fix
        final_state = research_agent_app.invoke(
            initial_state,
            config={"recursion_limit": 50}
        )

        print(f"\n{C_CYAN}================================================================={C_RESET}")
        print(f"{C_GREEN}>> WORKFLOW COMPLETE: FINAL REPORT GENERATED <<{C_RESET}")
        print(f"{C_CYAN}================================================================={C_RESET}")
        print(final_state.get('final_report', 'Report generation failed or was skipped.'))
        print(f"{C_CYAN}================================================================={C_RESET}")

    except Exception as e:
        print(f"\n{C_RED}================================================================={C_RESET}")
        print(f"{C_RED}>> WORKFLOW FAILED <<{C_RESET}")
        print(f"{C_RED}================================================================={C_RESET}")
        print(f"An unexpected error occurred: {e}")
        # Print a concise state summary upon failure to aid debugging
        print("\n--- State Summary at Failure ---")
        # Assuming you can access the failed state, otherwise this is a placeholder:
        # print(f"Last Agent: {last_agent_run}")
        # print(f"Raw Data Count: {len(initial_state.get('raw_tool_data', []))}")